{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba1ee40",
   "metadata": {},
   "source": [
    "### Yolo Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040b0078",
   "metadata": {},
   "source": [
    "#### A. Pre-trained Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dcb9e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24de14e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'models\\pretrained\\yolov8n.pt': 100% ━━━━━━━━━━━━ 6.2MB 94.4MB/s 0.1s\n",
      "Ultralytics 8.3.231  Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti, 16303MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.00.0 ms, read: 1025.4391.4 MB/s, size: 177.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\datasets\\coco\\labels\\val2017.cache... 4952 images, 48 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 5000/5000 310.5Kit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 79/79 3.2it/s 24.7s0.3ss\n",
      "                   all       5000      36335      0.632      0.476      0.521      0.372\n",
      "                person       2693      10777      0.753      0.673      0.745      0.515\n",
      "               bicycle        149        314      0.685      0.394      0.455      0.265\n",
      "                   car        535       1918      0.644      0.516      0.562      0.365\n",
      "            motorcycle        159        367      0.708      0.583       0.66      0.414\n",
      "              airplane         97        143      0.811      0.782      0.839      0.653\n",
      "                   bus        189        283      0.743      0.643      0.738       0.62\n",
      "                 train        157        190      0.799      0.776      0.834      0.648\n",
      "                 truck        250        414      0.553      0.401      0.435      0.295\n",
      "                  boat        121        424      0.582      0.296      0.377      0.211\n",
      "         traffic light        191        634      0.644      0.349       0.41      0.212\n",
      "          fire hydrant         86        101      0.849      0.703      0.774      0.615\n",
      "             stop sign         69         75      0.684       0.64      0.695      0.632\n",
      "         parking meter         37         60       0.63        0.5      0.558      0.441\n",
      "                 bench        235        411      0.562       0.26      0.295      0.194\n",
      "                  bird        125        427       0.68      0.358      0.425      0.276\n",
      "                   cat        184        202      0.775      0.827      0.856      0.651\n",
      "                   dog        177        218      0.656      0.702      0.731      0.596\n",
      "                 horse        128        272      0.698      0.647      0.691      0.524\n",
      "                 sheep         65        354      0.607      0.667      0.661       0.46\n",
      "                   cow         87        372      0.692      0.593      0.678      0.489\n",
      "              elephant         89        252        0.7      0.833      0.821      0.629\n",
      "                  bear         49         71      0.846      0.775      0.841      0.688\n",
      "                 zebra         85        266      0.806      0.797      0.882      0.656\n",
      "               giraffe        101        232      0.857      0.828      0.887      0.684\n",
      "              backpack        228        371       0.49      0.158        0.2      0.102\n",
      "              umbrella        174        407      0.629      0.504      0.541      0.361\n",
      "               handbag        292        540      0.481      0.128      0.168     0.0858\n",
      "                   tie        145        252      0.688      0.361      0.431      0.267\n",
      "              suitcase        105        299      0.571      0.441      0.504      0.345\n",
      "               frisbee         84        115      0.747      0.774      0.768      0.583\n",
      "                  skis        120        241        0.6      0.324       0.37      0.192\n",
      "             snowboard         49         69      0.503      0.333      0.397      0.268\n",
      "           sports ball        169        260      0.714      0.438      0.474      0.333\n",
      "                  kite         91        327      0.606       0.52      0.559      0.381\n",
      "          baseball bat         97        145      0.579      0.388      0.401      0.222\n",
      "        baseball glove        100        148      0.663      0.473      0.513      0.305\n",
      "            skateboard        127        179      0.713      0.603      0.664      0.451\n",
      "             surfboard        149        267      0.626      0.479      0.509      0.311\n",
      "         tennis racket        167        225      0.705      0.622      0.671      0.398\n",
      "                bottle        379       1013      0.605       0.39      0.456      0.298\n",
      "            wine glass        110        341      0.659      0.352       0.42      0.269\n",
      "                   cup        390        895      0.578      0.441      0.489       0.35\n",
      "                  fork        155        215      0.583      0.318      0.391      0.265\n",
      "                 knife        181        325      0.499      0.163      0.173      0.106\n",
      "                 spoon        153        253      0.413      0.146       0.16     0.0988\n",
      "                  bowl        314        623      0.588      0.494      0.529      0.391\n",
      "                banana        103        370      0.549      0.323      0.372      0.232\n",
      "                 apple         76        236      0.412      0.242      0.227      0.156\n",
      "              sandwich         98        177      0.559       0.48       0.46      0.348\n",
      "                orange         85        285       0.44      0.419      0.368      0.281\n",
      "              broccoli         71        312      0.472       0.34      0.368      0.208\n",
      "                carrot         81        365      0.442       0.29      0.307      0.189\n",
      "               hot dog         51        125       0.76      0.432      0.497      0.364\n",
      "                 pizza        153        284      0.652      0.613      0.659      0.503\n",
      "                 donut         62        328      0.574      0.497      0.514      0.407\n",
      "                  cake        124        310      0.532      0.393      0.436      0.291\n",
      "                 chair        580       1771      0.581      0.343      0.404      0.258\n",
      "                 couch        195        261      0.592      0.556      0.587      0.434\n",
      "          potted plant        172        342      0.516      0.386      0.382      0.225\n",
      "                   bed        149        163       0.55      0.552       0.59      0.429\n",
      "          dining table        501        695      0.524       0.44      0.435      0.292\n",
      "                toilet        149        179      0.717      0.737      0.773      0.641\n",
      "                    tv        207        288       0.77      0.635      0.713      0.553\n",
      "                laptop        183        231      0.687      0.662      0.702      0.581\n",
      "                 mouse         88        106      0.624      0.643      0.709      0.533\n",
      "                remote        145        283      0.433      0.223      0.272      0.162\n",
      "              keyboard        106        153      0.595      0.606      0.649      0.483\n",
      "            cell phone        214        262      0.549      0.355      0.401      0.281\n",
      "             microwave         54         55      0.616      0.545      0.643      0.513\n",
      "                  oven        115        143      0.636      0.448       0.51      0.348\n",
      "               toaster          8          9      0.725      0.222      0.436      0.316\n",
      "                  sink        187        225      0.565      0.445      0.509       0.34\n",
      "          refrigerator        101        126      0.659      0.595      0.652      0.512\n",
      "                  book        230       1129      0.489      0.112      0.197     0.0972\n",
      "                 clock        204        267      0.746      0.607      0.668       0.46\n",
      "                  vase        137        274      0.588      0.453      0.453      0.321\n",
      "              scissors         28         36      0.687      0.333      0.332      0.278\n",
      "            teddy bear         94        190      0.673      0.558      0.606      0.418\n",
      "            hair drier          9         11          1          0    0.00493    0.00377\n",
      "            toothbrush         34         57      0.401      0.193      0.238      0.166\n",
      "Speed: 0.4ms preprocess, 0.8ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Saving C:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\metrics\\yolov8n_coco_pretrained6\\predictions.json...\n",
      "\n",
      "Evaluating faster-coco-eval mAP using C:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\metrics\\yolov8n_coco_pretrained6\\predictions.json and C:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\datasets\\coco\\annotations\\instances_val2017.json...\n",
      "Evaluate annotation type *bbox*\n",
      "COCOeval_opt.evaluate() finished...\n",
      "DONE (t=4.20s).\n",
      "Accumulating evaluation results...\n",
      "COCOeval_opt.accumulate() finished...\n",
      "DONE (t=0.00s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.374\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.526\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.406\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.188\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.411\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.536\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.321\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.533\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.589\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.374\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.654\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.769\n",
      " Average Recall     (AR) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.812\n",
      " Average Recall     (AR) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.639\n",
      "Results saved to \u001b[1mC:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\metrics\\yolov8n_coco_pretrained6\u001b[0m\n",
      "Ultralytics 8.3.231  Python-3.12.12 torch-2.9.0+cu128 CPU (AMD Ryzen 5 9600X 6-Core Processor)\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\pretrained\\yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 22...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.75...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.5s, saved as 'models\\pretrained\\yolov8n.onnx' (12.3 MB)\n",
      "\n",
      "Export complete (0.6s)\n",
      "Results saved to \u001b[1mC:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\models\\pretrained\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\pretrained\\yolov8n.onnx imgsz=640  \n",
      "Validate:        yolo val task=detect model=models\\pretrained\\yolov8n.onnx imgsz=640 data=coco.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models\\\\pretrained\\\\yolov8n.onnx'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"models/pretrained/yolov8n.pt\")\n",
    "\n",
    "# Model Valudation\n",
    "model.val(\n",
    "    data=\"coco.yaml\", \n",
    "    imgsz=640, \n",
    "    batch=64,\n",
    "    project=\"metrics\",\n",
    "    name=\"yolov8n_coco_pretrained\")\n",
    "\n",
    "# Export Model Weights\n",
    "model.export(\n",
    "    format=\"onnx\",\n",
    "    imgsz=640,\n",
    "    project=\"models/pretrained\",\n",
    "    name=\"yolov8n_pretrained_onnx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19404400",
   "metadata": {},
   "source": [
    "#### B. Quantization Floating Point 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff48ebd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.231  Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti, 16303MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\pretrained\\yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 20...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.75...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.4s, saved as 'models\\pretrained\\yolov8n.onnx' (12.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.14.1.48.post1...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as models\\pretrained\\yolov8n.engine\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success  13.9s, saved as 'models\\pretrained\\yolov8n.engine' (7.9 MB)\n",
      "\n",
      "Export complete (14.0s)\n",
      "Results saved to \u001b[1mC:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\models\\pretrained\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\pretrained\\yolov8n.engine imgsz=640 half \n",
      "Validate:        yolo val task=detect model=models\\pretrained\\yolov8n.engine imgsz=640 data=coco.yaml half \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models\\\\pretrained\\\\yolov8n.engine'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = YOLO(\"models/pretrained/yolov8n.pt\")\n",
    "\n",
    "model.export(\n",
    "    format=\"engine\",\n",
    "    half=True,\n",
    "    imgsz=640,\n",
    "    project=\"models/exports\",\n",
    "    name=\"yolov8n_fp16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c2f0972c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.231  Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti, 16303MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\pretrained\\fp16_yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 20...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.75...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.4s, saved as 'models\\pretrained\\fp16_yolov8n.onnx' (12.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.14.1.48.post1...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building FP16 engine as models\\pretrained\\fp16_yolov8n.engine\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success  13.4s, saved as 'models\\pretrained\\fp16_yolov8n.engine' (7.8 MB)\n",
      "\n",
      "Export complete (13.5s)\n",
      "Results saved to \u001b[1mC:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\models\\pretrained\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\pretrained\\fp16_yolov8n.engine imgsz=640 half \n",
      "Validate:        yolo val task=detect model=models\\pretrained\\fp16_yolov8n.engine imgsz=640 data=coco.yaml half \n",
      "Visualize:       https://netron.app\n",
      "WARNING TensorRT requires GPU export, automatically assigning device=0\n",
      "Ultralytics 8.3.231  Python-3.12.12 torch-2.9.0+cu128 CUDA:0 (NVIDIA GeForce RTX 5070 Ti, 16303MiB)\n",
      "YOLOv8n summary (fused): 72 layers, 3,151,904 parameters, 0 gradients, 8.7 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'models\\pretrained\\int8_yolov8n.pt' with input shape (1, 3, 640, 640) BCHW and output shape(s) (1, 84, 8400) (6.2 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 20...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.75...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  0.5s, saved as 'models\\pretrained\\int8_yolov8n.onnx' (12.3 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m starting export with TensorRT 10.14.1.48.post1...\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m collecting INT8 calibration images from 'data=coco.yaml'\n",
      "Fast image access  (ping: 0.00.0 ms, read: 1319.5246.1 MB/s, size: 147.0 KB)\n",
      "\u001b[KScanning C:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\datasets\\coco\\labels\\val2017.cache... 4952 images, 48 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 5000/5000 5.0Mit/s 0.0s\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m input \"images\" with shape(1, 3, 640, 640) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m output \"output0\" with shape(1, 84, 8400) DataType.FLOAT\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m building INT8 engine as models\\pretrained\\int8_yolov8n.engine\n",
      "\u001b[34m\u001b[1mTensorRT:\u001b[0m export success  519.5s, saved as 'models\\pretrained\\int8_yolov8n.engine' (50.1 MB)\n",
      "\n",
      "Export complete (519.6s)\n",
      "Results saved to \u001b[1mC:\\Users\\EMILIO\\Desktop\\0. Cursos\\4. Advanced Computer Vision\\EdgeYoloSamPipeline\\models\\pretrained\u001b[0m\n",
      "Predict:         yolo predict task=detect model=models\\pretrained\\int8_yolov8n.engine imgsz=640 int8 \n",
      "Validate:        yolo val task=detect model=models\\pretrained\\int8_yolov8n.engine imgsz=640 data=coco.yaml int8 \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'models\\\\pretrained\\\\int8_yolov8n.engine'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"models/pretrained/fp16_yolov8n.pt\")\n",
    "\n",
    "model.export(\n",
    "    format=\"engine\",\n",
    "    half=True,\n",
    "    imgsz=640,\n",
    "    project=\"models/exports\",\n",
    "    name=\"yolov8n_fp16\"\n",
    ")\n",
    "\n",
    "\n",
    "model = YOLO(\"models/pretrained/int8_yolov8n.pt\")\n",
    "\n",
    "model.export(\n",
    "    format=\"engine\",\n",
    "    int8=True,\n",
    "    data=\"coco.yaml\",\n",
    "    imgsz=640,\n",
    "    project=\"models/exports\",\n",
    "    name=\"yolov8n_int8\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_yolosam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
